import type { OpenaiChatMessage } from './OpenaiChatMessage'

export interface OpenaiChatResponse {

  /** A unique identifier for the chat completion. */
  id: string

  /** The object type, which is always 'chat.completion'. */
  object: 'chat.completion'

  /** The Unix timestamp (in seconds) of when the chat completion was created. */
  created: number

  /** The model used for the chat completion. */
  model: string

  /**
   * The choices generated by the chat completion. Can be more than one if `n`
   * is greater than 1.
   */
  choices: Choice[]

  /** Usage statistics for the completion request. */
  usage: Usage

  /**
   * The service tier used for processing the request. This field is only
   * included if the `service_tier` parameter is specified in the request.
   */
  service_tier?: string

  /**
   * This fingerprint represents the backend configuration that the model runs
   * with. Can be used in conjunction with the `seed` request parameter to
   * understand when backend changes have been made that might impact determinism.
   */
  system_fingerprint: string
}

interface Choice {

  /** The index of the choice in the list of choices. */
  index: number

  /** The message object generated by the model. */
  message: OpenaiChatMessage

  /**
   * The reason the model stopped generating tokens. This will be `stop` if the
   * model hit a natural stop point or a provided stop sequence, `length` if the
   * maximum number of tokens specified in the request was reached, `content_filter`
   * if content was omitted due to a flag from our content filters, `tool_calls` if
   * the model called a tool, or `function_call` (deprecated) if the model called a
   * function.
   */
  finish_reason: 'content_filter' | 'function_call' | 'length' | 'stop' | 'tool_calls'

  /** Log probability information for the choice. */
  logprobs?: LogProbs | null
}

interface LogProbs {

  /** A list of message content tokens with log probability information. */
  content?: TokenLogProbs[] | null

  /** A list of message refusal tokens with log probability information. */
  refusal?: TokenLogProbs[] | null
}

interface TokenLogProbs {

  /** The token. */
  token: string

  /**
   * The log probability of this token, if it is within the top 20 most likely
   * tokens. Otherwise, the value -9999.0 is used to signify that the token is very
   * unlikely.
   */
  logprob: number

  /**
   * A list of integers representing the UTF-8 bytes representation of the token.
   * Useful in instances where characters are represented by multiple tokens and
   * their byte representations must be combined to generate the correct text
   * representation. Can be null if there is no bytes representation for the token.
   */
  bytes?: number[]

  /**
   * List of the most likely tokens and their log probabilities at this token
   * position. In rare cases, there may be fewer than the number of requested
   * `top_logprobs` returned.
   */
  top_logprobs?: TopLogProb[] | null
}

interface TopLogProb {

  /** The token. */
  token: string

  /**
   * The log probability of this token, if it is within the top 20 most likely
   * tokens. Otherwise, the value -9999.0 is used to signify that the token is very
   * unlikely.
   */
  logprob: number

  /**
   * A list of integers representing the UTF-8 bytes representation of the token.
   * Can be null if there is no bytes representation for the token.
   */
  bytes?: number[]
}

interface Usage {

  /** Number of tokens in the generated completion. */
  completion_tokens: number

  /** Number of tokens in the prompt. */
  prompt_tokens: number

  /** Total number of tokens used in the request (prompt + completion). */
  total_tokens: number

  /** Breakdown of tokens used in a completion. */
  completion_tokens_details?: CompletionTokensDetails

  /** Breakdown of tokens used in the prompt. */
  prompt_tokens_details?: PromptTokensDetails
}

interface CompletionTokensDetails {

  /** Audio input tokens generated by the model. */
  audio_tokens?: number

  /** Tokens generated by the model for reasoning. */
  reasoning_tokens?: number
}

interface PromptTokensDetails {

  /** Audio input tokens present in the prompt. */
  audio_tokens?: number

  /** Cached tokens present in the prompt. */
  cached_tokens?: number
}

export interface AudioData {

  /** Unique identifier for this audio response.  */
  id: string

  /**
   * The Unix timestamp (in seconds) for when this audio response will no longer
   * be accessible on the server for use in multi-turn conversations.
   */
  expires_at: number

  /**
   * Base64 encoded audio bytes generated by the model, in the format specified
   * in the request.
   */
  data: string

  /** Transcript of the audio generated by the model.  */
  transcript: string
}
