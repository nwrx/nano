import type { LanguageModelInferenceResponse } from '../types'
import { defineNode } from '@nwrx/core'
import { defineResultSchema } from '@nwrx/core'
import { languageModel } from '../categories'
import { languageModelInstance, number, string } from '../types'

interface InferenceResult extends LanguageModelInferenceResponse {
  tokensPerSecond: number
}

export const inference = defineNode({
  kind: 'inference',
  name: 'Inference',
  icon: 'https://api.iconify.design/carbon:ai.svg',
  description: 'Generates a completion based on a language model.',
  category: languageModel,

  dataSchema: {
    model: {
      name: 'Model',
      control: 'socket',
      type: languageModelInstance,
      description: 'The language model used to generate the completion.',
    },
    prompt: {
      type: string,
      name: 'Prompt',
      control: 'socket',
      description: 'The message to generate a completion for.',
    },
    frequency_penalty: {
      type: number,
      control: 'slider',
      name: 'Frequency Penalty',
      description: 'Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\'s likelihood to repeat the same line verbatim.',
      sliderMin: -2,
      sliderMax: 2,
      sliderStep: 0.1,
      defaultValue: 0,
    },
    logit_bias: {
      type: number,
      control: 'slider',
      name: 'Logit Bias',
      description: 'Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.',
      defaultValue: 0,
      sliderMin: -100,
      sliderMax: 100,
      sliderStep: 1,
    },
    max_completion_tokens: {
      type: number,
      control: 'slider',
      name: 'Max Tokens',
      description: 'The maximum number of tokens to generate the completion.',
      sliderMin: 0,
      sliderMax: 2048,
      sliderStep: 64,
      defaultValue: 1024,
    },
    temperature: {
      type: number,
      control: 'slider',
      name: 'Temperature',
      description: 'The sampling temperature for the completion.',
      sliderMin: 0.1,
      sliderMax: 2,
      sliderStep: 0.1,
      defaultValue: 0.7,
    },
  },

  resultSchema: defineResultSchema<InferenceResult>({
    completion: {
      type: string,
      name: 'Completion',
      description: 'The generated completion based on the prompt.',
    },
    tokensTotal: {
      type: number,
      name: 'Total Tokens',
      description: 'The total number of tokens used to generate the completion.',
    },
    tokensCompletion: {
      type: number,
      name: 'Completion Tokens',
      description: 'The number of tokens used in the completion.',
    },
    tokensPrompt: {
      type: number,
      name: 'Prompt Tokens',
      description: 'The number of tokens used in the prompt.',
    },
    tokensPerSecond: {
      type: number,
      name: 'Tokens Per Second',
      description: 'The number of tokens generated per second.',
    },
    fingerprint: {
      type: string,
      name: 'Fingerprint',
      description: 'The system fingerprint of the completion.',
    },
  }),

  process: async({ data }) => {
    const { model, prompt, max_completion_tokens: maxCompletionTokens, temperature } = data

    // --- Generate the completion based on the model and prompt.
    const start = Date.now()
    const body = model.getBody({ prompt, maxCompletionTokens, temperature })
    const response = await fetch(model.url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${model.token}`,
      },
      body: JSON.stringify(body),
    })

    // --- Handle the response status.
    if (!response.ok) {
      const errorObject = await response.json().catch(() => {}) as { error: { message: string } } | undefined
      throw new Error(errorObject?.error?.message ?? `Could not generate the completion: ${response.statusText}`)
    }

    // --- Return the completion from the response.
    const end = Date.now()
    const responseData = await response.json() as unknown
    const completion = model.getCompletion(responseData)
    const tokensPerSecond = Math.round(completion.tokensTotal / ((end - start) / 1000))
    return { ...completion, tokensPerSecond }
  },
})
