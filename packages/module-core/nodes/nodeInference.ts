import type { InferInput, InferOutput } from '@nwrx/core'
import type { ObjectLike } from '@unshared/types'
import { defineInputSchema, defineNode, defineOutputSchema } from '@nwrx/core'
import { categoryLanguageModel } from '../categories'
import { languageModel, languageModelTool, number, string } from '../types'

/** The data that is passed to the inference node. */
export type InferenceData = InferInput<typeof INFERENCE_INPUT_SCHEMA>

/** The result of the inference process. */
export type InferenceResult = InferOutput<typeof INFERENCE_OUTPUT_SCHEMA>

const INFERENCE_INPUT_SCHEMA = defineInputSchema({
  model: {
    type: languageModel,
    name: 'Model',
    control: 'socket',
    description: 'The language model used to generate the completion.',
  },
  tools: {
    type: languageModelTool,
    name: 'Tools',
    control: 'socket',
    description: 'The tools used to generate the completion.',
    isOptional: true,
    isIterable: true,
  },
  prompt: {
    type: string,
    name: 'Prompt',
    control: 'socket',
    description: 'The message to generate a completion for.',
  },
  temperature: {
    type: number,
    control: 'slider',
    name: 'Temperature',
    description: 'The sampling temperature for the completion.',
    sliderMin: 0.1,
    sliderMax: 2,
    sliderStep: 0.1,
    defaultValue: 1,
    isOptional: true,
  },
  frequencyPenalty: {
    type: number,
    control: 'slider',
    name: 'Frequency Penalty',
    description: 'Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\'s likelihood to repeat the same line verbatim.',
    sliderMin: -2,
    sliderMax: 2,
    sliderStep: 0.1,
    defaultValue: 0,
    isOptional: true,
  },
  logitBias: {
    type: number,
    control: 'slider',
    name: 'Logit Bias',
    description: 'Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.',
    defaultValue: 0,
    sliderMin: -100,
    sliderMax: 100,
    sliderStep: 1,
    isOptional: true,
  },
  maxCompletionTokens: {
    type: number,
    control: 'slider',
    name: 'Max Tokens',
    description: 'The maximum number of tokens to generate the completion.',
    sliderMin: 0,
    sliderMax: 2048,
    sliderStep: 64,
    defaultValue: 1024,
    isOptional: true,
  },
  seed: {
    type: number,
    name: 'Seed',
    control: 'slider',
    description: 'The seed used to generate the completion.',
    defaultValue: 0,
    sliderMin: 0,
    sliderMax: 100,
    isOptional: true,
  },
})

const INFERENCE_OUTPUT_SCHEMA = defineOutputSchema({
  id: {
    type: string,
    name: 'Identifier',
    description: 'The unique identifier of the completion.',
  },
  completion: {
    type: string,
    name: 'Completion',
    description: 'The generated completion based on the prompt.',
  },
})

export const nodeInference = defineNode({
  kind: 'core/inference',
  name: 'Inference',
  icon: 'https://api.iconify.design/majesticons:sparkles-line.svg',
  description: 'Generates a completion based on a language model.',

  category: categoryLanguageModel,
  inputSchema: INFERENCE_INPUT_SCHEMA,
  outputSchema: INFERENCE_OUTPUT_SCHEMA,

  process: async({ input }) => {
    const { model, tools } = input
    const { url, token, getBody, onData, onError } = model
    const body = getBody(input)

    let canResume = false
    function resume() {
      canResume = true
    }

    async function call(name: string, parameters: ObjectLike) {
      if (!tools) throw new Error('The tools were not provided.')
      const tool = tools.find(tool => tool.name === name)
      if (!tool) throw new Error(`The tool "${name}" was not provided.`)
      return await tool.call(parameters)
    }

    // --- Send the request to the model API. Retry up to 10 times to handle any tool calls
    // --- that may be requested by the model. Resume the completion process after handling
    // --- the tool calls.
    for (let i = 0; i < 10; i++) {
      const response = await fetch(url, {
        method: 'POST',
        body: JSON.stringify(body),
        headers: {
          'Authorization': `Bearer ${token}`,
          'Content-Type': 'application/json',
        },
      })

      // --- If an error occurred, handle it and throw it back with the error message
      // --- extracted from the response body. This is specific to the `LanguageModel`
      // --- interface and requires the `onError` function to be defined.
      if (!response.ok) {
        throw typeof onError === 'function'
          ? await onError(response)
          : new Error(response.statusText)
      }

      // --- If the response is OK, extract the data from the response and pass it to the
      // --- `onData` function to handle the completion. If the `resume` function not called
      // --- in the `onData` function, break the loop and return the result.
      canResume = false
      const data = await response.json() as ObjectLike
      const result = await onData({ body, data, call, resume })
      if (result) return result
      if (!canResume) break
    }

    // --- Abort after the maximum number of iterations to prevent infinite loops.
    throw new Error('The inference process did not complete in the specified number of iterations.')
  },
})
