import type { ObjectLike } from '@nwrx/core'
import type { LanguageModelResult } from './languageModel'
import { defineNode, defineResultSchema } from '@nwrx/core'
import { defineDataSchema } from '@nwrx/core'
import { number, string } from '../types'
import { languageModel } from './languageModel'
import { languageModelCategory } from './languageModelCategory'
import { languageModelTool } from './languageModelTool'

export const inference = defineNode({
  kind: 'inference',
  name: 'Inference',
  icon: 'https://api.iconify.design/carbon:ai.svg',
  description: 'Generates a completion based on a language model.',
  category: languageModelCategory,

  dataSchema: defineDataSchema({
    model: {
      name: 'Model',
      control: 'socket',
      type: languageModel,
      description: 'The language model used to generate the completion.',
    },
    tools: {
      type: languageModelTool,
      name: 'Tools',
      control: 'socket',
      description: 'The tools used to generate the completion.',
      isOptional: true,
      isArray: true,
    },
    prompt: {
      type: string,
      name: 'Prompt',
      control: 'socket',
      description: 'The message to generate a completion for.',
    },
    frequencyPenalty: {
      type: number,
      control: 'slider',
      name: 'Frequency Penalty',
      description: 'Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\'s likelihood to repeat the same line verbatim.',
      sliderMin: -2,
      sliderMax: 2,
      sliderStep: 0.1,
      defaultValue: 0,
    },
    logitBias: {
      type: number,
      control: 'slider',
      name: 'Logit Bias',
      description: 'Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.',
      defaultValue: 0,
      sliderMin: -100,
      sliderMax: 100,
      sliderStep: 1,
    },
    maxCompletionTokens: {
      type: number,
      control: 'slider',
      name: 'Max Tokens',
      description: 'The maximum number of tokens to generate the completion.',
      sliderMin: 0,
      sliderMax: 2048,
      sliderStep: 64,
      defaultValue: 1024,
    },
    temperature: {
      type: number,
      control: 'slider',
      name: 'Temperature',
      description: 'The sampling temperature for the completion.',
      sliderMin: 0.1,
      sliderMax: 2,
      sliderStep: 0.1,
      defaultValue: 0.7,
    },
  }),

  resultSchema: defineResultSchema<LanguageModelResult>({
    completion: {
      type: string,
      name: 'Completion',
      description: 'The generated completion based on the prompt.',
    },
    tokensTotal: {
      type: number,
      name: 'Total Tokens',
      description: 'The total number of tokens used to generate the completion.',
    },
    tokensCompletion: {
      type: number,
      name: 'Completion Tokens',
      description: 'The number of tokens used in the completion.',
    },
    tokensPrompt: {
      type: number,
      name: 'Prompt Tokens',
      description: 'The number of tokens used in the prompt.',
    },
    id: {
      type: string,
      name: 'Identifier',
      description: 'The unique identifier of the completion.',
    },
  }),

  process: async({ data }) => {
    const { model, tools } = data
    const { url, token, getBody, onData, onError } = model
    const body = getBody(data) as ObjectLike

    let canResume = false
    function resume() {
      canResume = true
    }

    async function call(name: string, data: unknown) {
      if (!tools) throw new Error('The tools were not provided.')
      const tool = tools.find(tool => tool.name === name)
      if (!tool) throw new Error(`The tool "${name}" was not provided.`)
      return await tool.call(data)
    }

    // --- Send the request to the model API. Retry up to 10 times to handle any tool calls
    // --- that may be requested by the model. Resume the completion process after handling
    // --- the tool calls.
    for (let i = 0; i < 10; i++) {
      const response = await fetch(url, {
        method: 'POST',
        body: JSON.stringify(body),
        headers: {
          'Authorization': `Bearer ${token}`,
          'Content-Type': 'application/json',
        },
      })

      // --- If an error occurred, handle it and throw it back with the error message
      // --- extracted from the response body. This is specific to the `LanguageModel`
      // --- interface and requires the `onError` function to be defined.
      if (!response.ok) {
        const message = onError ? await onError(response) : response.statusText
        throw new Error(message)
      }

      // --- If the response is OK, extract the data from the response and pass it to the
      // --- `onData` function to handle the completion. If the `resume` function not called
      // --- in the `onData` function, break the loop and return the result.
      canResume = false
      const data = await response.json() as ObjectLike

      console.log('-'.repeat(80))
      console.log(`Iteration ${i + 1}`)
      console.log(JSON.stringify(data, null, 2))
      console.log('-'.repeat(80))
      console.log(JSON.stringify(body, null, 2))

      const result = await onData({ body, data, call, resume })
      if (result) return result
      if (!canResume) break
    }

    // --- Abort after the maximum number of iterations to prevent infinite loops.
    throw new Error('The inference process did not complete in the specified number of iterations.')
  },
})
