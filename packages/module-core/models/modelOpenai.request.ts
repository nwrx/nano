import type { JSONSchema4 } from 'json-schema'
import type { AudioData, Message } from './modelOpenai.types'

export interface OpenaiChatRequest {

  /**
   * A list of messages comprising the conversation so far. Depending on the
   * model you use, different message types (modalities) are supported, like text,
   * images, and audio.
   */
  messages: Message[]

  /**
   * ID of the model to use. See the model endpoint compatibility table for
   * details on which models work with the Chat API.
   */
  model: string

  /**
   * Whether or not to store the output of this chat completion request for use
   * in our model distillation or evals products. @default false
   */
  store?: boolean

  /**
   * Developer-defined tags and values used for filtering completions in the
   * dashboard.
   */
  metadata?: Record<string, any>

  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on
   * their existing frequency in the text so far, decreasing the model's likelihood
   * to repeat the same line verbatim.
   *
   * See more information about frequency and presence penalties.
   *
   * @default 0
   */
  frequency_penalty?: number

  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   * Accepts a JSON object that maps tokens (specified by their token ID in the
   * tokenizer) to an associated bias value from -100 to 100. Mathematically, the
   * bias is added to the logits generated by the model prior to sampling. The exact
   * effect will vary per model, but values between -1 and 1 should decrease or
   * increase likelihood of selection; values like -100 or 100 should result in a ban
   * or exclusive selection of the relevant token.
   */
  logit_bias?: Record<string, number>

  /**
   * Whether to return log probabilities of the output tokens or not. If true,
   * returns the log probabilities of each output token returned in the content of
   * message.
   *
   * @default false
   */
  logprobs?: boolean

  /**
   * An integer between 0 and 20 specifying the number of most likely tokens to
   * return at each token position, each with an associated log probability.
   * `logprobs` must be set to true if this parameter is used.
   */
  top_logprobs?: number

  /**
   * An upper bound for the number of tokens that can be generated for a
   * completion, including visible output tokens and reasoning tokens.
   */
  max_completion_tokens?: number

  /**
   * How many chat completion choices to generate for each input message. Note
   * that you will be charged based on the number of generated tokens across all of
   * the choices. Keep `n` as `1` to minimize costs.
   *
   * @default 1
   */
  n?: number

  /**
   * Output types that you would like the model to generate for this request.
   * Most models are capable of generating text, which is the default: `["text"]`.
   *
   * The `gpt-4o-audio-preview` model can also be used to generate audio. To
   * request that this model generate both text and audio responses, you can use:
   * `["text", "audio"]`
   */
  modalities?: Array<'audio' | 'text'>

  /**
   * Parameters for audio output. Required when audio output is requested with
   * `modalities: ["audio"]`. Learn more.
   */
  audio?: AudioData

  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on
   * whether they appear in the text so far, increasing the model's likelihood to
   * talk about new topics.
   *
   * See more information about frequency and presence penalties.
   *
   * @default 0
   */
  presence_penalty?: number

  /**
   * An object specifying the format that the model must output. Compatible with
   * GPT-4o, GPT-4o mini, GPT-4 Turbo and all GPT-3.5 Turbo models newer than
   * `gpt-3.5-turbo-1106`.
   *
   * Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
   * Outputs which ensures the model will match your supplied JSON schema. Learn more
   * in the Structured Outputs guide.
   *
   * Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
   * message the model generates is valid JSON.
   *
   * Important: when using JSON mode, you must also instruct the model to produce
   * JSON yourself via a system or user message. Without this, the model may generate
   * an unending stream of whitespace until the generation reaches the token limit,
   * resulting in a long-running and seemingly "stuck" request. Also note that the
   * message content may be partially cut off if `finish_reason="length"`, which
   * indicates the generation exceeded `max_tokens` or the conversation exceeded the
   * max context length.
   */
  response_format?: ResponseFormat

  /**
   * This feature is in Beta. If specified, our system will make a best effort to
   * sample deterministically, such that repeated requests with the same seed and
   * parameters should return the same result. Determinism is not guaranteed, and you
   * should refer to the `system_fingerprint` response parameter to monitor changes
   * in the backend.
   */
  seed?: number

  /**
   * Specifies the latency tier to use for processing the request. This parameter
   * is relevant for customers subscribed to the scale tier service:
   *
   * - If set to 'auto', and the Project is Scale tier enabled, the system will
   * utilize scale tier credits until they are exhausted.
   * - If set to 'auto', and the Project is not Scale tier enabled, the request will
   * be processed using the default service tier with a lower uptime SLA and no
   * latency guarantee.
   * - If set to 'default', the request will be processed using the default service
   * tier with a lower uptime SLA and no latency guarantee.
   * - When not set, the default behavior is 'auto'.
   *
   * When this parameter is set, the response body will include the `service_tier`
   * utilized.
   *
   * @default 'auto'
   */
  service_tier?: 'auto' | 'default'

  /**
   * Up to 4 sequences where the API will stop generating further tokens.
   *
   * @default null
   */
  stop?: string | string[]

  /**
   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be
   * sent as data-only server-sent events as they become available, with the stream
   * terminated by a `data: [DONE]` message. Example Python code.
   *
   * @default false
   */
  stream?: boolean

  /**
   * Options for streaming response. Only set this when you set `stream: true`.
   *
   * @default null
   */
  stream_options?: StreamOptions

  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8
   * will make the output more random, while lower values like 0.2 will make it more
   * focused and deterministic.
   *
   * We generally recommend altering this or `top_p` but not both.
   *
   * @default 1
   */
  temperature?: number

  /**
   * An alternative to sampling with temperature, called nucleus sampling, where
   * the model considers the results of the tokens with top_p probability mass. So
   * 0.1 means only the tokens comprising the top 10% probability mass are considered.
   *
   * We generally recommend altering this or `temperature` but not both.
   *
   * @default 1
   */
  top_p?: number

  /**
   * A list of tools the model may call. Currently, only functions are supported
   * as a tool. Use this to provide a list of functions the model may generate JSON
   * inputs for. A max of 128 functions are supported.
   */
  tools?: Tool[]

  /**
   * Controls which (if any) tool is called by the model. `none` means the model
   * will not call any tool and instead generates a message. `auto` means the model
   * can pick between generating a message or calling one or more tools. `required`
   * means the model must call one or more tools. Specifying a particular tool via
   * `ToolChoiceFunction` forces the model to call that tool.
   *
   * `none` is the default when no tools are present. `auto` is the default if tools
   * are present.
   */
  tool_choice?: 'auto' | 'none' | 'required' | ToolChoiceFunction

  /**
   * Whether to enable parallel function calling during tool use.
   *
   * @default true
   */
  parallel_tool_calls?: boolean

  /**
   * A unique identifier representing your end-user, which can help OpenAI to
   * monitor and detect abuse. Learn more.
   */
  user?: string
}

/** Specifies the format that the model must output. */
type ResponseFormat =
  | { type: 'json_object' }
  | { type: 'json_schema'; json_schema: JsonSchema }
  | { type: 'text' }

/** JSON schema for the response format. */
interface JsonSchema {

  /**
   * A description of what the response format is for, used by the model
   * to determine how to respond in the format.
   */
  description?: string

  /**
   * The name of the response format. Must be a-z, A-Z, 0-9, or contain
   * underscores and dashes, with a maximum length of 64.
   */
  name: string

  /** The schema for the response format, described as a JSON Schema object. */
  schema?: object

  /**
   * Whether to enable strict schema adherence when generating the output.
   * If true, the model will follow the exact schema defined in the `schema` field.
   * Only a subset of JSON Schema is supported when `strict` is `true`. Learn more
   * about Structured Outputs in the function calling guide.
   *
   * @default false
   */
  strict?: boolean
}

/** Options for streaming response. */
interface StreamOptions {

  /**
   * If set, an additional chunk will be streamed before the `data: [DONE]` message.
   * The `usage` field on this chunk shows the token usage statistics for the entire
   * request, and the choices field will always be an empty array. All other chunks
   * will also include a usage field, but with a null value.
   */
  include_usage?: boolean
}

/** Definition of a tool the model may call. */
interface Tool {

  /** The type of the tool. Currently, only 'function' is supported. */
  type: 'function'

  /** The function that the model can call. */
  function: ToolFunctionDefinition
}

/** Definition of a function the model may call. */
interface ToolFunctionDefinition {

  /**
   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
   * underscores and dashes, with a maximum length of 64.
   */
  name: string

  /**
   * A description of what the function does, used by the model to choose when
   * and how to call the function.
   */
  description?: string

  /**
   * The parameters the functions accepts, described as a JSON Schema object.
   * See the guide for examples, and the JSON Schema reference for documentation
   * about the format.
   *
   * Omitting `parameters` defines a function with an empty parameter list.
   */
  parameters?: JSONSchema4

  /**
   * Whether to enable strict schema adherence when generating the function call.
   * If set to true, the model will follow the exact schema defined in the
   * `parameters` field. Only a subset of JSON Schema is supported when `strict` is
   * true. Learn more about Structured Outputs in the function calling guide.
   *
   * @default false
   */
  strict?: boolean
}

/** Controls which specific tool is called by the model. */
interface ToolChoiceFunction {
  type: 'function'
  function: { name: string }
}
